# DSL-23-1-modeling-Stable-Diffusion

### Member: 남승우, 신소연, 안세정, 정건우

## Overview

### task
> * Reverse the typical direction of a generative text-to-image model(Stable Diffusion 2.0)
>   * Create the model that can predict the text prompt with (text, image) pairs generated by Stable Diffusion 2.0
>   * Make the prediction and compare the cosine similarity with the (text, image) pairs

## Model

### BLIP-2
<img width="600" alt="drawing" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/blip2_architecture.jpg">
#### pretrained models(Colab) : blip2-flan-t5-xl

#### pretrained models(Kaggle) : blip2-opt-2.7b  - limited RAM capacity

#### Usage
```
!pip install transformers #(Colab)
from transformers import Blip2Processor, BlipForConditionalGeneration
processor = Blip2Processor.from_pretrained('Salesforce/blip-flan-t5-xl')
model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-flan-t5-xl')
```
### CLIP
<img src="https://github.com/openai/CLIP/raw/main/CLIP.png" alt="CLIP" style="max-width: 100%;">

#### pretrained models : CLIP-ViT-H-14-laion-s32B-b79k

#### Usage
```
!pip install open_clip_torch
!pip install clip-interrogator==0.6.0
model, _, preprocess = open_clip.create_model_and_transforms('ViT-H-14', pretrained = 'laion2b-s32B-b79k)
tokenizer = open_clip.get_tokenizer('ViT-H-14')
```

### Results


## File Description

### models 
1. data
- images
- sample_submission.csv
- prompts.txt
2. modules
- BLIP2_CLIP_model_c.ipynb
- BLIP2_CLIP_model_k.ipynb

### results
- submission.csv
